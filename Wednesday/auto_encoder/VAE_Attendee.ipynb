{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComputeFest 2019\n",
    "## Convolutional Autoencoders for Image Manipulation\n",
    "\n",
    "Pavlos Protopapas, Vincent Casser, Camilo Fosco $\\bullet$ Harvard University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble using Google Colab, contact a workshop member and we will use the following link for setting up: https://canvas.harvard.edu/courses/53717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Important: Remember to switch runtime to GPU first.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download required code and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Harvard-IACS/2019-computefest.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"2019-computefest/Wednesday/auto_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import both Keras and the Tensorflow backend. In addition, some common libraries are used for I/O, image and data processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Reshape, Dense, Lambda, Flatten\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import glob\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.io\n",
    "import PIL\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import imageio\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major definition here will be our network architecture for the VAE. It follows an encoder-decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_encoder_block(x, num_filters):  \n",
    "    \"\"\"\n",
    "    Todo: Define two sequential 2D convolutional layers (Conv2D) with the following properties:\n",
    "          - num_filters many filters\n",
    "          - kernel_size 3\n",
    "          - activation \"relu\"\n",
    "          - padding \"same\"\n",
    "          - kernel_initializer \"he_normal\"\n",
    "          Also define a 2D max pooling layer (MaxPooling2D) (you can keep default arguments).\n",
    "    \"\"\"\n",
    "    \n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    \n",
    "    \n",
    "    # TODO: Write code here.\n",
    "    \n",
    "    \n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_decoder_block(x, num_filters):\n",
    "    \"\"\"\n",
    "    Todo: Define one 2D upsampling layer (UpSampling2D) (you can keep default arguments).\n",
    "          Also, define three sequential 2D convolutional layers (Conv2D) with the following properties:\n",
    "          - num_filters many filters\n",
    "          - kernel_size 3\n",
    "          - activation \"relu\"\n",
    "          - padding \"same\"\n",
    "          - kernel_initializer \"he_normal\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    \n",
    "    \n",
    "    # TODO: Write code here.\n",
    "    \n",
    "    \n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_net(variational, height, width, batch_size, latent_dim, conditioning_dim=0,\n",
    "               start_filters=8):\n",
    "    \"\"\"Defines a (variational) encoder-decoder architecture.\n",
    "    \n",
    "    Args:\n",
    "        variational: Whether a variational autoencoder should be defined.\n",
    "        height: The height of the image input and output.\n",
    "        width: The width of the image input and output.\n",
    "        batch_size: The batchsize that is used during training. Must also be used for inference on the encoder side.\n",
    "        latent_dim: The dimension of the latent space.\n",
    "        conditioning_dim: The dimension of the space of variables to condition on. Can be zero for an unconditional VAE.\n",
    "        start_filters: The number of filters to start from. Multiples of this value are used across the network. Can be used\n",
    "            to change model capacity.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of keras models for full VAE, encoder part and decoder part only.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the inputs.\n",
    "    inputs = Input((height, width, 3))\n",
    "    if conditioning_dim > 0:\n",
    "        # Define conditional VAE. Note that this is usually not the preferred way\n",
    "        # of incorporating the conditioning information in the encoder.\n",
    "        condition = Input([conditioning_dim])\n",
    "        condition_up = Dense(height * width)(condition)\n",
    "        condition_up = Reshape([height, width, 1])(condition_up)\n",
    "        inputs_new = Concatenate(axis=3)([inputs, condition_up])\n",
    "    else:\n",
    "        inputs_new = inputs\n",
    "    \n",
    "    # Define the encoder.\n",
    "    eblock1 = define_encoder_block(inputs_new, start_filters)\n",
    "    eblock2 = define_encoder_block(eblock1, start_filters*2)\n",
    "    eblock3 = define_encoder_block(eblock2, start_filters*4)\n",
    "    eblock4 = define_encoder_block(eblock3, start_filters*8)\n",
    "    _, *shape_spatial = eblock4.get_shape().as_list()\n",
    "    eblock4_flat = Flatten()(eblock4)\n",
    "    \n",
    "    if not variational:\n",
    "        z = Dense(latent_dim)(eblock4_flat)\n",
    "    else:\n",
    "        # Perform the sampling.\n",
    "        def sampling(args):\n",
    "            \"\"\"Samples latent variable from a normal distribution using the given parameters.\"\"\"\n",
    "            z_mean, z_log_sigma = args\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n",
    "            return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "        \n",
    "        z_mean = Dense(latent_dim)(eblock4_flat)\n",
    "        z_log_sigma = Dense(latent_dim)(eblock4_flat)\n",
    "        z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "    \n",
    "    if conditioning_dim > 0:\n",
    "        z_ext = Concatenate()([z, condition])\n",
    "\n",
    "    # Define the decoder.\n",
    "    inputs_embedding = Input([latent_dim + conditioning_dim])\n",
    "    embedding = Dense(np.prod(shape_spatial), activation='relu')(inputs_embedding)\n",
    "    embedding = Reshape(eblock4.shape.as_list()[1:])(embedding)\n",
    "    \n",
    "    dblock1 = define_decoder_block(embedding, start_filters*8)\n",
    "    dblock2 = define_decoder_block(dblock1, start_filters*4)\n",
    "    dblock3 = define_decoder_block(dblock2, start_filters*2)\n",
    "    dblock4 = define_decoder_block(dblock3, start_filters)\n",
    "    output = Conv2D(3, 1, activation = 'tanh')(dblock4)\n",
    "    \n",
    "    # Define the models.\n",
    "    decoder = Model(input = inputs_embedding, output = output)\n",
    "    if conditioning_dim > 0:\n",
    "        encoder_with_sampling = Model(input = [inputs, condition], output = z)\n",
    "        encoder_with_sampling_ext = Model(input = [inputs, condition], output = z_ext)\n",
    "        vae_out = decoder(encoder_with_sampling_ext([inputs, condition]))\n",
    "        vae = Model(input = [inputs, condition], output = vae_out)\n",
    "    else:\n",
    "        encoder_with_sampling = Model(input = inputs, output = z)\n",
    "        vae_out = decoder(encoder_with_sampling(inputs))\n",
    "        vae = Model(input = inputs, output = vae_out)\n",
    "    \n",
    "    # Define the VAE loss.\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        \"\"\"Defines the VAE loss functions as a combination of MSE and KL-divergence loss.\"\"\"\n",
    "        mse_loss = K.mean(keras.losses.mse(x, x_decoded_mean), axis=(1,2)) * height * width\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "        return mse_loss + kl_loss\n",
    "    \n",
    "    if variational:\n",
    "        vae.compile(loss=vae_loss, optimizer='adam')\n",
    "    else:\n",
    "        vae.compile(loss='mse', optimizer='adam')    \n",
    "        \n",
    "    print('done,', vae.count_params(), 'parameters.')\n",
    "    return vae, encoder_with_sampling, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some functions to make encoding and decoding easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_image(img, conditioning, encoder, height, width, batch_size):\n",
    "    '''Encodes an image that is given in RGB-channel order with value range of [0, 255].\n",
    "    \n",
    "    Args:\n",
    "        img: The image input. If shapes differ from (height, width), it will be resized.\n",
    "        conditoning: The set of values to condition on, if any. Can be None.\n",
    "        encoder: The keras encoder model to use.\n",
    "        height: The target image height.\n",
    "        width: The target image width.\n",
    "        batch_size: The batchsize that the encoder expects.\n",
    "        \n",
    "    Returns:\n",
    "        The latent representation of the input image.\n",
    "    '''\n",
    "    if img.shape[0] != height or img.shape[1] != width:\n",
    "        img = skimage.transform.resize(img, (height, width))\n",
    "    img_single = np.expand_dims(img, axis=0)\n",
    "    img_single = img_single.astype(np.float32)\n",
    "    img_single = np.repeat(img_single, batch_size, axis=0)\n",
    "    if conditioning is None:\n",
    "        z = encoder.predict(img_single)\n",
    "    else:\n",
    "        z = encoder.predict([img_single, np.repeat(np.expand_dims(conditioning, axis=0), batch_size, axis=0)])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_embedding(z, conditioning, decoder):\n",
    "    '''Decodes the given representation into an image.\n",
    "    \n",
    "    Args:\n",
    "        z: The latent representation.\n",
    "        conditioning: The set of values to condition on, if any. Can be None.\n",
    "        decoder: The keras decoder model to use.\n",
    "    '''\n",
    "    if z.ndim < 2:\n",
    "        z = np.expand_dims(z, axis=0) # Single-batch\n",
    "    if conditioning is not None:\n",
    "        z = np.concatenate((z, np.repeat(np.expand_dims(conditioning, axis=0), z.shape[0], axis=0)), axis=1)\n",
    "    return decoder.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(folder):\n",
    "    vae.load_weights(folder + '/vae.w')\n",
    "    encoder.load_weights(folder + '/encoder.w')\n",
    "    decoder.load_weights(folder + '/decoder.w')\n",
    "    \n",
    "def save_weights(folder):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    vae.save_weights(folder + '/vae.w')\n",
    "    encoder.save_weights(folder + '/encoder.w')\n",
    "    decoder.save_weights(folder + '/decoder.w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the MNIST data by downsampling it and scaling the intensity ranges. Note that we could have also used binary crossentropy loss here, as MNIST is often used in binarized form. However, we treat the intensity values as floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = utils.load_mnist(target_height=32, target_width=32)\n",
    "# We work on a subset here, to make training on CPU's feasible.\n",
    "indices_train = np.random.choice(list(range(x_train.shape[0])), size=30000, replace=False)\n",
    "x_train = x_train[indices_train]\n",
    "y_train = y_train[indices_train]\n",
    "indices_test = np.random.choice(list(range(x_test.shape[0])), size=2000, replace=False)\n",
    "x_test = x_test[indices_test]\n",
    "y_test = y_test[indices_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unconditional AE: Application to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the autoencoder using the definitions above, then we train it for a single epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, encoder, decoder = define_net(\n",
    "    False, 32, 32, batch_size=8, latent_dim=2, start_filters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae.fit(x_train, x_train, batch_size=8, verbose=1, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can load pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_weights(folder='models/mnist_ae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_weights(folder='models/mnist_ae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the latent representation of all the MNIST test images and their groundtruth label. As we can see, the clusters are already grouped together quite nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=8)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_demo = x_test[np.random.randint(x_test.shape[0])]\n",
    "pred = vae.predict(np.expand_dims(x_demo, axis=0))\n",
    "plt.imshow(x_demo)\n",
    "plt.show()\n",
    "plt.imshow(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unconditional VAE: Application to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the VAE network using the definitions above, then we train it for a single epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae, encoder, decoder = define_net(\n",
    "    True, 32, 32, batch_size=8, latent_dim=2, start_filters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae.fit(x_train, x_train, batch_size=8, verbose=1, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can load pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_weights(folder='models/mnist_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_weights(folder='models/mnist_vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After just one epoch, we already get promising results. Below, we plot the latent representation of all the MNIST test images and their groundtruth label. As we can see, the clusters are already grouped together quite nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=8)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also render parts of the manifold by varying the latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.display_manifold(decoder,\n",
    "                           32,\n",
    "                           32,\n",
    "                           bound_x=5,\n",
    "                           bound_y=5,\n",
    "                           base_vec=np.array([0,0]),\n",
    "                           desc_x='latent_dim1',\n",
    "                           desc_y='latent_dim2',\n",
    "                           file_out='rendering_mnist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conditional VAE: Application to CelebA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the face dataset CelebA, we will use a conditional VAE. We first define the hyperparameters to use for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VARIATIONAL = True\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "BATCH_SIZE = 16\n",
    "LATENT_DIM = 16\n",
    "START_FILTERS = 32\n",
    "CONDITIONING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a large dataset, we also define a custom data generator to avoid caching the whole set in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, files, batch_size, target_height, target_width, conditioning_dim=0, conditioning_data=None):\n",
    "        '''\n",
    "        Intializes the custom generator.\n",
    "        \n",
    "        Args:\n",
    "            files: The list of paths to images that should be fed to the network.\n",
    "            batch_size: The batchsize to use.\n",
    "            target_height: The target image height. If different, the images will be resized.\n",
    "            target_width: The target image width. If different, the images will be resized.\n",
    "            conditioning_dim: The dimension of the conditional variable space. Can be 0.\n",
    "            conditioning_data: Optional dictionary that maps from the filename to the data to be\n",
    "                conditioned on. Data must be numeric. Can be None. Otherwise, len must be equal to\n",
    "                conditioning_dim.\n",
    "        '''\n",
    "        self.files = files\n",
    "        self.batch_size = batch_size\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "        self.conditioning_dim = conditioning_dim\n",
    "        self.conditioning_data = conditioning_data\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        '''Shuffle list of files after each epoch.'''\n",
    "        np.random.shuffle(self.files)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        cur_files = self.files[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(cur_files)\n",
    "        return X, y\n",
    "    \n",
    "    def __data_generation(self, cur_files):\n",
    "        X = np.empty(shape=(self.batch_size, self.target_height, self.target_width, 3))\n",
    "        Y = np.empty(shape=(self.batch_size, self.target_height, self.target_width, 3))\n",
    "        if self.conditioning_data != None:\n",
    "            C = np.empty(shape=(self.batch_size, self.conditioning_dim))\n",
    "        \n",
    "        for i, file in enumerate(cur_files):\n",
    "            img = skimage.io.imread(file)\n",
    "            if img.shape[0] != self.target_height or img.shape[1] != self.target_width:\n",
    "                img = skimage.transform.resize(img, (self.target_height, self.target_width)) # Resize.\n",
    "            img = img.astype(np.float32) / 255.\n",
    "            X[i] = img\n",
    "            Y[i] = img\n",
    "            if self.conditioning_data != None:\n",
    "                C[i] = self.conditioning_data[os.path.basename(file)]\n",
    "                \n",
    "        if self.conditioning_data != None:\n",
    "            return [X, C], Y\n",
    "        else:\n",
    "            return X, Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.files) / self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the annotation data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image files.\n",
    "files = glob.glob('celeba/img_align_celeba/*.jpg')\n",
    "print(len(files), 'images found.')\n",
    "\n",
    "df = utils.load_celeba('celeba/list_attr_celeba.txt')\n",
    "columns = df.columns\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the dataframe into a dictionary, mapping from filename to the individual image attributes. We then instantiate the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = {}\n",
    "selected_conditionals = list(columns[1:])\n",
    "for i, row in df.iterrows():\n",
    "    dd[row['Filename']] = [int(row[c]) for c in selected_conditionals]\n",
    "\n",
    "gen = CustomDataGenerator(files=files, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          target_height=HEIGHT, \n",
    "                          target_width=WIDTH, \n",
    "                          conditioning_dim=len(selected_conditionals),\n",
    "                          conditioning_data=dd if CONDITIONING else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the conditional VAE and start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae, encoder, decoder = define_net(variational=VARIATIONAL,\n",
    "                                   height=HEIGHT, \n",
    "                                   width=WIDTH, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   latent_dim=LATENT_DIM,\n",
    "                                   conditioning_dim=len(selected_conditionals) if CONDITIONING else 0, \n",
    "                                   start_filters=START_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vae.fit_generator(gen, verbose=1, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can load pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_weights(folder='models/celeba_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_weights(folder='models/celeba_vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples. First, we will select a random image from the CelebA dataset, and read the related annotation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_file = np.random.choice(files)\n",
    "file_id = os.path.basename(rnd_file)\n",
    "init_meta = dd[file_id]\n",
    "img = skimage.io.imread(rnd_file)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will encode the image into its latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encode_image(img.astype(np.float32) / 255., np.array(init_meta), encoder, HEIGHT, WIDTH, BATCH_SIZE)\n",
    "print('latent sample:\\n', z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding the latent representation should result in a face with somewhat similar characterics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = decode_embedding(z, init_meta, decoder)\n",
    "plt.imshow(ret[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is interesting: We can manipulate the embedding to change the facial structure. We can modify both our conditionals, as well as the latent variables. Here, we plot sliders to make the process interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Interactive widgets (i.e. this cell) will not work in Colab.\n",
    "\n",
    "def render(**params):\n",
    "    perturb_latent = np.zeros(shape=(LATENT_DIM))\n",
    "    for i in range(LATENT_DIM):\n",
    "        perturb_latent[i] = params['latent' + str(i)]\n",
    "\n",
    "    c = []\n",
    "    for k,v in params.items():\n",
    "        if not k.startswith('latent'):\n",
    "            c.append(v)\n",
    "    ret = decode_embedding(z[0] + perturb_latent, conditioning=np.array(c), decoder=decoder)[0]\n",
    "    ret = np.clip(ret, 0, 1)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(ret)\n",
    "    \n",
    "lower, upper = -10, 10\n",
    "params = {}\n",
    "for i, c in enumerate(selected_conditionals):\n",
    "    params[c] = widgets.FloatSlider(min=lower,max=upper,step=0.1,value=init_meta[i], layout=widgets.Layout(width='70%', height='20px'))\n",
    "for c in ['latent' + str(i) for i in range(LATENT_DIM)]:\n",
    "    params[c] = widgets.FloatSlider(min=lower,max=upper,step=0.1,value=0,layout=widgets.Layout(width='70%', height='20px'))\n",
    "    \n",
    "interactive_plot = interactive(render, **params)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '200px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the previous cell didn't work, in the next cell you'll be able to manually change some values of the conditional vector, and give your generated images some specific attriburtes. \n",
    "### Play around with the different values and see what it generates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the following variables to change the face generated below.\n",
    "# Set them to numbers between roughly -10 and 10.\n",
    "# Large values might seem unrealistic, so try with small numbers first.\n",
    "# If you set them to None, they will take the default values.\n",
    "\n",
    "smiling = None # e.g. change this to -3 or 3 and see the results!\n",
    "male = None\n",
    "no_beard = None\n",
    "attractive = None\n",
    "bald = None\n",
    "chubby = None\n",
    "eyeglasses = None\n",
    "young = None\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Don't change the code below.\n",
    "# It introduces the values you defined in the conditional vector\n",
    "# that is used by the VAE.\n",
    "\n",
    "meta = init_meta.copy()\n",
    "meta[2] = attractive if attractive else init_meta[2]\n",
    "meta[4] = bald if bald else init_meta[4]\n",
    "meta[13] = chubby if chubby else init_meta[13]\n",
    "meta[15] = eyeglasses if eyeglasses else init_meta[15]\n",
    "meta[20] = male if male else init_meta[20]\n",
    "meta[24] = no_beard if no_beard else init_meta[24]\n",
    "meta[31] = smiling if smiling else init_meta[31]\n",
    "meta[39] = young if young else init_meta[39]\n",
    "\n",
    "ret = decode_embedding(z, meta, decoder)\n",
    "plt.imshow(ret[0])\n",
    "plt.show()\n",
    "\n",
    "print('Attractive:', meta[2])\n",
    "print('Bald:', meta[4])\n",
    "print('Chubby:', meta[13])\n",
    "print('Eyeglasses:', meta[15])\n",
    "print('Male:', meta[20])\n",
    "print('No_Beard:', meta[24])\n",
    "print('Smiling:', meta[31])\n",
    "print('Young:', meta[39])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our MNIST example, we can also visualize a grid of samples by just manipulating two of the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = 'Male' # you can change these to other attributes such as No_Beard or Young\n",
    "dim2 = 'Smiling'\n",
    "base_vec = np.array(list(z[0]) + meta)\n",
    "rendering, _ = utils.display_manifold(\n",
    "    decoder, \n",
    "    HEIGHT, \n",
    "    WIDTH, \n",
    "    base_vec, \n",
    "    bound_x=15, \n",
    "    bound_y=15, \n",
    "    axis_x=LATENT_DIM + selected_conditionals.index(dim1), \n",
    "    axis_y=LATENT_DIM + selected_conditionals.index(dim2), \n",
    "    n=10,\n",
    "    desc_x = dim1,\n",
    "    desc_y = dim2,\n",
    "    file_out = 'rendering_celeba_' + dim1.lower() + '_' + dim2.lower() + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can create image sequences when manipulating a certain dimension. We save the result as an animated GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 'Smiling' # you can change these to other attributes such as No_Beard or Young\n",
    "utils.generate_gif(decoder, \n",
    "                   height=HEIGHT, \n",
    "                   width=WIDTH,\n",
    "                   base_vec=np.array(list(base_vec)), \n",
    "                   axis=LATENT_DIM + selected_conditionals.index(dim),\n",
    "                   total_frames=30,\n",
    "                   degree=5,\n",
    "                   file_out='animation_' + dim.lower() + '.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem of variational autoencoders is their tendency to produce blurry, non-photorealistic outputs. We can observe the same behaviour, especially on natural images (CelebA). GANs, with their explicit formulation of a discriminator network that learns to differentiate what is photorealistic to humans and what is not, tend to produce even nicer-looking results.\n",
    "\n",
    "If we wanted to use this model to change the characteristics of a particular face based on a photo, we would have to either: \n",
    "(1) estimate all the conditional variables ourselves to feed them into the encoder and decoder, or\n",
    "(2) change the model formulation such that the encoder simultaneously predicts conditional attributes such as gender and age.\n",
    "\n",
    "With the current formulation, however, we can already nicely synthesize specific faces, and customize them to fit our needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
